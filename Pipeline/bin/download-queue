#!/bin/bash
#
# This script is re-useable: it requires a directory with (sorted)
# files, which each contain one url to be downloaded into the
# cache directory.

set -e

function log() { printf "[%s] %s\n" "$(date +'%F %T')" "$*"; }

if [ $# -ne 4 ]
then echo "Required: $0 <queue> <todo> <cache> <cache-size>" >&2
     exit 1
fi
QUEUE="$1"
PATHS_TODO="$2"
CACHE_DIR="$3"
CACHE_SIZE="$4"

if [ ! -d "$PATHS_TODO" ]
then log "No plan for queue $QUEUE in $PATHS_TODO"
     exit 0
fi

log "** Downloader for $QUEUE into $CACHE_DIR, max $CACHE_SIZE"


[ -d "$CACHE_DIR" ] || mkdir -p "$CACHE_DIR"
CACHED=$(ls "$CACHE_DIR" | wc -l)
if [ $CACHED -ge $CACHE_SIZE ]
then log "Cache is full (size $CACHED, max $CACHE_SIZE)"
     exit 0
fi

[ -d "$LOCKS" ] || mkdir -p "$LOCKS"
LOCK_DEST="$LOCKS/$QUEUE-$$-locker"
touch "$LOCK_DEST"

ls "$PATHS_TODO" | while read BATCH_LABEL
    do TODO="$PATHS_TODO/$BATCH_LABEL"

       LOCK="$LOCKS/$BATCH_LABEL.lock"
       if ! ln -s "$LOCK_DEST" "$LOCK" 2>/dev/null
       then log "$BATCH_LABEL is currently locked.  Next..."
            continue
       fi

       AWS_PATH=$(cat "$TODO")
       DEST="$CACHE_DIR/$BATCH_LABEL-$QUEUE.warc.gz"

       DOWNLOAD="$DEST.downloading"
       rm -f "$DOWNLOAD"    # broken downloads

       log "Collecting $AWS_PATH into $DOWNLOAD"
       $AWS_CP "$AWS_PATH" "$DOWNLOAD" >/dev/null

       SIZE=$(ls -lhk "$DOWNLOAD" | perl -ane 'print $F[4]')
       log "Got $BATCH_LABEL, size $SIZE"

       log "$BATCH_LABEL-$QUEUE available in $DEST"
       mv "$DOWNLOAD" "$DEST"

       rm "$TODO"
       rm "$LOCK"

       # Downloaded enough?
       CACHED=$(ls "$CACHE_DIR" | wc -l)
       [ $CACHED -lt $CACHE_SIZE ] || break

    done

rm $LOCK_DEST
